{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpCtuTTuKeTqvJ+Br3TMUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anstrebkov/anstrebkov/blob/main/parse_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akZLEfxnQbbG",
        "outputId": "04e7701e-3d11-4fae-b051-dc43f92bec18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit (from -r requirements.txt (line 1))\n",
            "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting langchain (from -r requirements.txt (line 2))\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-groq (from -r requirements.txt (line 3))\n",
            "  Downloading langchain_groq-0.2.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting selenium (from -r requirements.txt (line 4))\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.12.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.9.4)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.1)\n",
            "Collecting python-dotenv (from -r requirements.txt (line 8))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.32.3)\n",
            "Collecting groq==0.9.0 (from -r requirements.txt (line 10))\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.24.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq==0.9.0->-r requirements.txt (line 10)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq==0.9.0->-r requirements.txt (line 10)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq==0.9.0->-r requirements.txt (line 10))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq==0.9.0->-r requirements.txt (line 10)) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq==0.9.0->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq==0.9.0->-r requirements.txt (line 10)) (4.12.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit->-r requirements.txt (line 1)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (16.1.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (13.9.2)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit->-r requirements.txt (line 1))\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->-r requirements.txt (line 1)) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit->-r requirements.txt (line 1))\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.12 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->-r requirements.txt (line 4)) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium->-r requirements.txt (line 4))\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->-r requirements.txt (line 4))\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium->-r requirements.txt (line 4)) (2024.8.30)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->-r requirements.txt (line 5)) (2.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 11)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 11)) (2024.6.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 11)) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (1.15.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.12.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq==0.9.0->-r requirements.txt (line 10)) (1.2.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (4.0.11)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq==0.9.0->-r requirements.txt (line 10))\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq==0.9.0->-r requirements.txt (line 10))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.12->langchain->-r requirements.txt (line 2))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2))\n",
            "  Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 2))\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq==0.9.0->-r requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq==0.9.0->-r requirements.txt (line 10)) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 2)) (3.1.1)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->-r requirements.txt (line 4))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium->-r requirements.txt (line 4))\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->-r requirements.txt (line 4))\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->-r requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.0.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain->-r requirements.txt (line 2))\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r requirements.txt (line 1)) (0.1.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (0.2.0)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: sortedcontainers, watchdog, python-dotenv, outcome, orjson, jsonpointer, h11, wsproto, trio, requests-toolbelt, pydeck, jsonpatch, httpcore, trio-websocket, httpx, selenium, langsmith, groq, streamlit, langchain-core, langchain-text-splitters, langchain-groq, langchain\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.4 langchain-core-0.3.12 langchain-groq-0.2.0 langchain-text-splitters-0.3.0 langsmith-0.1.136 orjson-3.10.9 outcome-1.3.0.post0 pydeck-0.9.1 python-dotenv-1.0.1 requests-toolbelt-1.0.0 selenium-4.25.0 sortedcontainers-2.4.0 streamlit-1.39.0 trio-0.27.0 trio-websocket-0.11.1 watchdog-5.0.3 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsxrAHBwWxUo",
        "outputId": "0f49b5f0-a521-483c-b24f-5fbb8592e126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 3s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "1 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerability\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPAGCzSjV8d_",
        "outputId": "b9cd05a1-fae4-48e0-f1d1-e2cd978051ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from scrape import scrape_website, extract_body_content, clean_body_content, split_dom_content\n",
        "from parse import parse_with_hf\n",
        "\n",
        "st.title(\"AI Web Scraper\")\n",
        "url = st.text_input(\"Enter Website URL\")\n",
        "\n",
        "if st.button(\"Scrape Website\"):\n",
        "    if url:\n",
        "        st.write(\"Scraping the website...\")\n",
        "        dom_content = scrape_website(url)\n",
        "        body_content = extract_body_content(dom_content)\n",
        "        cleaned_content = clean_body_content(body_content)\n",
        "        st.session_state.dom_content = cleaned_content\n",
        "\n",
        "        with st.expander(\"View DOM Content\"):\n",
        "            st.text_area(\"DOM Content\", cleaned_content, height=300)\n",
        "\n",
        "if \"dom_content\" in st.session_state:\n",
        "    parse_description = st.text_area(\"Describe what you want to parse\")\n",
        "    if st.button(\"Parse Content\"):\n",
        "        if parse_description:\n",
        "            st.write(\"Parsing the content...\")\n",
        "            dom_chunks = split_dom_content(st.session_state.dom_content)\n",
        "            parsed_result = parse_with_hf(dom_chunks, parse_description)\n",
        "            st.write(parsed_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "windrcCBW4IG",
        "outputId": "6983978a-fe4d-4ed6-bf34-d43271a1e0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.196.162.18:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://hip-gifts-fetch.loca.lt\n",
            "Connecting to Scraping Browser...\n",
            "2024-10-22 18:49:25.900 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 11, in <module>\n",
            "    dom_content = scrape_website(url)\n",
            "  File \"/content/scrape.py\", line 15, in scrape_website\n",
            "    driver.get(website)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 363, in get\n",
            "    self.execute(Command.GET, {\"url\": url})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 354, in execute\n",
            "    self.error_handler.check_response(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n",
            "    raise exception_class(message, screen, stacktrace)\n",
            "selenium.common.exceptions.WebDriverException: Message: unknown error: unhandled inspector error: {\"code\":\"block_brob\",\"message\":\"Requested URL is restricted in accordance with robots.txt. Ask your account manager to get full access for targeting this site (block_brob)\"}\n",
            "  (Session info: chrome=129.0.6668.59)\n",
            "\n",
            "Connecting to Scraping Browser...\n",
            "2024-10-22 18:50:03.688 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 11, in <module>\n",
            "    dom_content = scrape_website(url)\n",
            "  File \"/content/scrape.py\", line 15, in scrape_website\n",
            "    driver.get(website)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 363, in get\n",
            "    self.execute(Command.GET, {\"url\": url})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 354, in execute\n",
            "    self.error_handler.check_response(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n",
            "    raise exception_class(message, screen, stacktrace)\n",
            "selenium.common.exceptions.WebDriverException: Message: unknown error: unhandled inspector error: {\"code\":\"block_brob\",\"message\":\"Requested URL is restricted in accordance with robots.txt. Ask your account manager to get full access for targeting this site (block_brob)\"}\n",
            "  (Session info: chrome=129.0.6668.59)\n",
            "\n",
            "Connecting to Scraping Browser...\n",
            "Waiting captcha to solve...\n",
            "Captcha solve status: not_detected\n",
            "Navigated! Scraping page content...\n",
            "Connecting to Scraping Browser...\n",
            "Waiting captcha to solve...\n",
            "Captcha solve status: not_detected\n",
            "Navigated! Scraping page content...\n",
            "Connecting to Scraping Browser...\n",
            "Waiting captcha to solve...\n",
            "Captcha solve status: not_detected\n",
            "Navigated! Scraping page content...\n",
            "Connecting to Scraping Browser...\n",
            "Waiting captcha to solve...\n",
            "Captcha solve status: not_detected\n",
            "Navigated! Scraping page content...\n",
            "Connecting to Scraping Browser...\n",
            "Waiting captcha to solve...\n",
            "Captcha solve status: not_detected\n",
            "Navigated! Scraping page content...\n",
            "Connecting to Scraping Browser...\n",
            "2024-10-22 18:56:51.910 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 11, in <module>\n",
            "    dom_content = scrape_website(url)\n",
            "  File \"/content/scrape.py\", line 15, in scrape_website\n",
            "    driver.get(website)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 363, in get\n",
            "    self.execute(Command.GET, {\"url\": url})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 354, in execute\n",
            "    self.error_handler.check_response(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n",
            "    raise exception_class(message, screen, stacktrace)\n",
            "selenium.common.exceptions.WebDriverException: Message: unknown error: unhandled inspector error: {\"code\":\"block_brob\",\"message\":\"Requested URL is restricted in accordance with robots.txt. Ask your account manager to get full access for targeting this site (block_brob)\"}\n",
            "  (Session info: chrome=129.0.6668.59)\n",
            "\n",
            "Connecting to Scraping Browser...\n",
            "Waiting captcha to solve...\n",
            "Captcha solve status: not_detected\n",
            "Navigated! Scraping page content...\n",
            "Connecting to Scraping Browser...\n",
            "2024-10-22 18:58:18.612 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
            "    result = func()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 11, in <module>\n",
            "    dom_content = scrape_website(url)\n",
            "  File \"/content/scrape.py\", line 15, in scrape_website\n",
            "    driver.get(website)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 363, in get\n",
            "    self.execute(Command.GET, {\"url\": url})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/webdriver.py\", line 354, in execute\n",
            "    self.error_handler.check_response(response)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n",
            "    raise exception_class(message, screen, stacktrace)\n",
            "selenium.common.exceptions.WebDriverException: Message: unknown error: unhandled inspector error: {\"code\":\"block_brob\",\"message\":\"Requested URL is restricted in accordance with robots.txt. Ask your account manager to get full access for targeting this site (block_brob)\"}\n",
            "  (Session info: chrome=129.0.6668.59)\n",
            "\n",
            "Connecting to Scraping Browser...\n",
            "Waiting captcha to solve...\n",
            "Captcha solve status: not_detected\n",
            "Navigated! Scraping page content...\n",
            "Sending request for batch 1\n",
            "Response for batch 1: ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='| Название Продукта | Цена (₽) | Скидка (%) | Осталось (шт) | Бренд | Объем (литров) | Давление (бар) | Доставка |\\n|-------------------|-----------|------------|---------------|-------|----------------|----------------|-----------|\\n| Гидроаккумулятор для систем водоснабжения OASIS GN-24N 24 литра | 1,713 | 58 | - | OASIS | 24 | - | 27 октября |\\n| Гидроаккумулятор вертикальный ThermaTron 50 литров 1\" | 4,813 | 48 | - | ThermaTron | 50 | - | 25 октября |\\n| Гидроаккумулятор Waterstry 100 л вертикальный для систем холодного и горячего водоснабжения | 12,108 | 40 | - | Waterstry | 100 | - | 27 октября |\\n| Гидроаккумулятор Waterstry 100л с проходной мембраной Se.Fa. (Италия) | 14,645 | - | - | Waterstry | 100 | - | 27 октября |\\n| Гидроаккумулятор для систем водоснабжения ГА-50 50 л Вихрь / Бак для воды (50 л; 8 атм) | 4,759 | 68 | - | Вихрь | 50 | 8 | 25 октября |', tool_calls=None), logprobs=None)], created=1729623691, id='', model='Qwen/Qwen2.5-72B-Instruct', system_fingerprint='2.3.2-dev0-sha-28bb7ae', usage=ChatCompletionOutputUsage(completion_tokens=387, prompt_tokens=2469, total_tokens=2856))\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_generated/types/base.py:139: FutureWarning: Accessing 'ChatCompletionOutput' values through dict is deprecated and will be removed from version '0.25'. Use dataclass attributes instead.\n",
            "  warnings.warn(\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGhveottpxd4"
      },
      "outputs": [],
      "source": [
        "parse.py\n",
        "import os\n",
        "import random\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "\n",
        "# Set your Hugging Face API key\n",
        "client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
        "    \"Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
        "]\n",
        "\n",
        "def parse_with_hf(dom_chunks, parse_description):\n",
        "    parsed_results = []\n",
        "\n",
        "    # Loop through each chunk of DOM content\n",
        "    for i, chunk in enumerate(dom_chunks, start=1):\n",
        "        retries = 3\n",
        "        while retries > 0:\n",
        "            try:\n",
        "                # Set a random user agent\n",
        "                headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "\n",
        "                # Prepare the messages for the chat completion API\n",
        "                messages = [\n",
        "                    {\"role\": \"user\", \"content\": f\"Text: {chunk}\\n\\nTask: {parse_description}\"}\n",
        "                ]\n",
        "\n",
        "                # Call the Hugging Face API and stream the response\n",
        "                print(f\"Sending request for batch {i}\")\n",
        "                response = client.chat_completion(\n",
        "                    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "                    messages=messages,\n",
        "                    max_tokens=500,\n",
        "                    stream=False  # Set to False for debugging purposes\n",
        "                )\n",
        "\n",
        "                # Print the raw response for debugging\n",
        "                print(f\"Response for batch {i}: {response}\")\n",
        "\n",
        "                # Append the result to the list\n",
        "                if response:\n",
        "                    parsed_results.append(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "                break  # Break out of the retry loop if successful\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during Hugging Face API call: {e}\")\n",
        "                retries -= 1\n",
        "                if retries == 0:\n",
        "                    print(f\"Failed to parse batch {i} after 3 retries.\")\n",
        "                    parsed_results.append(\"\")  # Append an empty string if all retries fail\n",
        "\n",
        "    return \"\\n\".join(parsed_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7bqlDUvp5yn"
      },
      "outputs": [],
      "source": [
        "scrape.py\n",
        "from selenium.webdriver import Remote, ChromeOptions\n",
        "from selenium.webdriver.chromium.remote_connection import ChromiumRemoteConnection\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "AUTH = 'brd-customer-hl_215f6edb-zone-scraping_browser1:iv2f93a1jtjr'\n",
        "SBR_WEBDRIVER = f'https://{AUTH}@zproxy.lum-superproxy.io:9515'\n",
        "#wss://brd-customer-hl_215f6edb-zone-scraping_browser1:iv2f93a1jtjr@brd.superproxy.io:9222\n",
        "\n",
        "def scrape_website(website):\n",
        "    print(\"Connecting to Scraping Browser...\")\n",
        "    sbr_connection = ChromiumRemoteConnection(SBR_WEBDRIVER, \"goog\", \"chrome\")\n",
        "    with Remote(sbr_connection, options=ChromeOptions()) as driver:\n",
        "        driver.get(website)\n",
        "        print(\"Waiting captcha to solve...\")\n",
        "        solve_res = driver.execute(\n",
        "            \"executeCdpCommand\",\n",
        "            {\n",
        "                \"cmd\": \"Captcha.waitForSolve\",\n",
        "                \"params\": {\"detectTimeout\": 10000},\n",
        "            },\n",
        "        )\n",
        "        print(\"Captcha solve status:\", solve_res[\"value\"][\"status\"])\n",
        "        print(\"Navigated! Scraping page content...\")\n",
        "        html = driver.page_source\n",
        "        return html\n",
        "\n",
        "def extract_body_content(html_content):\n",
        "    \"\"\"\n",
        "    Extracts the entire body content from the HTML, including all <div>, <p>, <span>, etc.\n",
        "    \"\"\"\n",
        "    if html_content is None:\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    body_content = soup.body\n",
        "    if body_content:\n",
        "        return str(body_content)\n",
        "    return \"\"\n",
        "\n",
        "def clean_body_content(body_content):\n",
        "    \"\"\"\n",
        "    Cleans the body content by removing <script> and <style> tags, and then extracting\n",
        "    all text content (from <div>, <p>, <span>, etc.).\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(body_content, \"html.parser\")\n",
        "\n",
        "    # Remove script and style elements\n",
        "    for script_or_style in soup([\"script\", \"style\"]):\n",
        "        script_or_style.extract()\n",
        "\n",
        "    # Extract text from remaining elements\n",
        "    cleaned_content = soup.get_text(separator=\"\\n\")\n",
        "\n",
        "    # Remove excessive newlines or whitespaces\n",
        "    cleaned_content = \"\\n\".join(\n",
        "        line.strip() for line in cleaned_content.splitlines() if line.strip()\n",
        "    )\n",
        "\n",
        "    return cleaned_content\n",
        "\n",
        "def split_dom_content(dom_content, max_length=6000):\n",
        "    \"\"\"\n",
        "    Splits large content into smaller chunks to avoid issues with large inputs.\n",
        "    \"\"\"\n",
        "    return [\n",
        "        dom_content[i : i + max_length] for i in range(0, len(dom_content), max_length)\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMZcbW9lqq5q"
      },
      "outputs": [],
      "source": [
        "streamlit\n",
        "langchain\n",
        "langchain-groq\n",
        "selenium\n",
        "beautifulsoup4\n",
        "lxml\n",
        "html5lib\n",
        "python-dotenv\n",
        "requests\n",
        "groq==0.9.0\n"
      ]
    }
  ]
}